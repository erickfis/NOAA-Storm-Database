# Data Processing 


## Software and instructions for reproducibility

This study was conducted under the following software:

- `r R.version.string`
- Platform `r R.version$platform`
- OS: Linux Mint 18.1 Serena

Using the same versions under the same OS should guaranty reproducibility.

This script downloads the relevant raw data from NOAA repository, under ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/, and process it.

It is not necessary to keep the files downloaded after running this script because it writes the processed data to the file "data/harm.rds"

Then everytime it is runned, it checks for new data on the ftp server. If there is new data, it will download the files missing and raw process everything again.


## Raw data processing

In order to answer our questions, the original database needed to be treated from its raw form to a more useful format.




```{r libraries}

library(RCurl)
library(scales)
library(stringr)
library(data.table)
library(chron)
library(dplyr)
library(lubridate)
library(ggplot2)
library(rmarkdown)
library(RColorBrewer)
library(gridExtra)
library(grid)

```



```{r processamento, cache=FALSE}

# looks at ftp repository and list files
url <- "ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/"
arquivos.ftp <- getURL(url, ftp.use.epsv = FALSE,dirlistonly = TRUE) 
arquivos.ftp <- paste(strsplit(arquivos.ftp, "\n")[[1]], sep = "")
arquivos.ftp <- arquivos.ftp[grep("details", arquivos.ftp)]

tem.novidade <- 0 # start the trigger        

# Verifies if the ftp data is the same that the data that has already been used before.
# If not, set the download trigger and raw process trigger

if (file.exists("../data/lista.ftp.txt")) {
        data.previous <- readLines("../data/lista.ftp.txt")
                if (identical(data.previous,arquivos.ftp)) {
                        tem.novidade <- 0
                } else {
                        tem.novidade <- 1
                        writeLines(arquivos.ftp, "../data/lista.ftp.txt")
                }
        
} else {
        tem.novidade <- 1
        writeLines(arquivos.ftp, "../data/lista.ftp.txt")
}




# compares ftp repo to files already downloaded
arquivos.local <- dir("../data/")
arquivos.faltando <- arquivos.ftp[which(!(arquivos.ftp %in% arquivos.local))]


# if the lists of files are different, download new ones 

if(length(arquivos.faltando)>0 & tem.novidade ==1) {

                arquivo <- character()
                for (i in 1:length(arquivos.faltando)) {
                        arquivo <- paste0(url, arquivos.faltando[i])
                        download.file(arquivo, 
                                paste0("../data/", arquivos.faltando[i]),
                                method = "curl")
                }
}        

# check if processed data is present:

tem.rds <- file.exists("../data/harm.rds")

# if trigger is set, raw process again

if(tem.novidade==1|tem.rds==0) { # them process the data
        arquivos.local <- dir("../data/")
        arquivos.local <- arquivos.local[grep("details", arquivos.local)]

        dados <- fread(sprintf("gzip -dc %s | tr -d '\\000'",
                                paste0("../data/", arquivos.local[1])),
                        na.strings = "")

        for (i in 2:length(arquivos.local)) {
                dados <- bind_rows(
                        dados, fread(sprintf(
                                "gzip -dc %s | tr -d '\\000'",
                                paste0("../data/", arquivos.local[i]),
                                na.strings = "")
                                )
                        )
        }
        
# treating var names
        names(dados) <- gsub("_", ".", tolower(names(dados)))


# treat vars
# treating event types
        trata.eventos <- function(eventos) {
                eventos <- tolower(as.character(eventos))
                # first, need to see what are the event types
                # unique(eventos)
                # contagem <- sort(table(eventos))
                
                # them we create this list of terms
                
                lista.search <- c(
                        "dry",        
                        "fog",        
                        "wind",
                        "winter",
                        "slide",
                        "snow",
                        "flood",
                        "fld",
                        "cold|freez",
                        "hurricane",
                        "tornado",
                        "rain|precip",
                        "hail",
                        "heat|warm",
                        "tide",
                        "storm",
                        "record",
                        "blizzard",
                        "fire",
                        "funnel",
                        "surf")
                
                lista.replace <- c(
                        "drought",
                        "fog",        
                        "wind",
                        "winter",
                        "slide",
                        "snow",
                        "flood",
                        "flood",
                        "cold",
                        "hurricane",
                        "tornado",
                        "rain",
                        "hail",
                        "heat",
                        "tide",
                        "storm",
                        "record temperature",
                        "blizzard",
                        "fire",
                        "funnel",
                        "surf")
                
                
                for (i in 1:length(lista.search)) {
                        eventos[grepl(lista.search[i], eventos)] <- lista.replace[i]
                        
                }
                
                # lets group the events whose count is < 5 and call it "other"
                contagem <- sort(table(eventos))
                outros <- names(contagem[contagem<5])
                eventos[eventos %in% outros] <- "other"
                eventos
        }

# treating dates types
        arruma.data <- function(dia) {
        #adicionar prefixo 19 se menor que 99, 20 se > 99
        # 20## if year < 50, 19 if not
                condicao <- as.numeric(substr(dia,8,9))<50

                dia[condicao] <- paste0(
                        # day-month
                        substr(dia[condicao],1,7),
                        #full year
                        paste0("20", substr(dia[condicao],8,9)),
                        # hours
                        substr(dia[condicao],10,
                                nchar(dia[condicao])) 
                        )

                dia[!condicao] <- paste0(
                        # day-month
                        substr(dia[!condicao],1,7),
                        #full year
                        paste0("19", substr(dia[!condicao],8,9)), 
                        # hours
                        substr(dia[!condicao],10,
                                nchar(dia[!condicao])) 
                        )
                dia
        }

# Treating County Names
        treat.local <- function(cidades) {
                cidades <- tolower(str_trim(cidades))
                cidades <- gsub("_| |-", ".", cidades)
                
                # remove valley, coastal
                cidades <-
                gsub("valley|coastal|county", "", cidades)
                
                # remove ".."
                # cidades[grep("\\.\\.$", cidades)] <-
                #         gsub("\\.\\.", "", cidades[grep("\\.\\.$", cidades)])
                cidades[grep("\\.+", cidades)] <-
                        gsub("\\.+", ".", cidades[grep("\\.+", cidades)])
                cidades[grep("\\.$+", cidades)] <-
                        gsub("\\.+", "", cidades[grep("\\.$+", cidades)])
                
                # remove "> ( ,"
                cidades <-
                        sapply(strsplit(cidades, split = ">", fixed = TRUE),
                                function(x) (x[1]))
                cidades <-
                        sapply(strsplit(cidades, split = "(", fixed = TRUE),
                                function(x) (x[1]))
                cidades <-
                        sapply(strsplit(cidades, split = ",", fixed = TRUE),
                                function(x) (x[1]))
                cidades
        }

# fix exp data
        eval.dmg <- function(dmg) {
        # check for unique exp chars
        # exp <- str_extract(str_trim(harm.df$damage.crops), "[aA-zZ]+")
        # exp.unicos <- unique(exp)
        # exp <- str_extract(str_trim(harm.df$damage.property), "[aA-zZ]+")
        # exp.unicos <- unique(exp)  
                # prepare exp, for 10^exp        
                # remove white space and extract chars
                exp <- str_extract(str_trim(dmg), "[aA-zZ]+")
                exp[is.na(exp)] <- as.character(0) # NAs get 0
                exp[which(exp %in% c("+","?", "-"))] <- as.character(0)
                exp[which(exp %in% c("H","h"))] <- as.character(2)
                exp[which(exp %in% c("K","k"))] <- as.character(3)
                exp[which(exp %in% c("M","m"))] <- as.character(6)
                exp[which(exp %in% c("B","b"))] <- as.character(9)
                exp[which(exp %in% c("T","t"))] <- as.character(12)
                exp <- as.numeric(exp)
                
                # prepare numbers for number*10^exp
                number <- strsplit(dmg, split="[aA-zZ]", fixed=FALSE)
                number <- sapply(number, function(x) (x[1]))
                number <- as.numeric(number)
                
                # now evaluate them all
                dmg <- number*10^exp
                dmg
        }
        
        dados$begin.date.time <- arruma.data(dados$begin.date.time)
        dados$end.date.time <- arruma.data(dados$end.date.time)

        harm.df <- dados %>% 
                transmute(
                        event = trata.eventos(event.type),
                        magnitude,
                        day = as.Date(dmy_hms(begin.date.time,
                                        locale = "en_US.utf8"),
                                "%m/%d/%Y"),
                        duration = as.period(interval(
                                        dmy_hms(end.date.time, 
                                                locale = "en_US.utf8"),
                                        dmy_hms(begin.date.time, 
                                                locale = "en_US.utf8")
                                                )
                                        ),
                        state = treat.local(state), county = treat.local(cz.name),
                        fatalities = as.numeric(deaths.direct) + 
                                as.numeric(deaths.indirect),
                        injuries = as.numeric(injuries.direct) + 
                                as.numeric(injuries.indirect),
                        prop.ev = eval.dmg(damage.property),
                        crop.ev = eval.dmg(damage.crops)
                ) 

        rm(dados) # house cleanning
        
        # end of processing - save the data
        saveRDS(harm.df, "../data/harm.rds")

} else { # just loads processed data
        harm.df <- readRDS("../data/harm.rds")
}

```


The necessary transformations were:

- sanitized var names
- evaluated duration of events, however they are not useful 
- evaluated damages values according to multipliers provided
- sanitized and grouped similar events: strong snow, heavy snow and light snow all became just "snow"
- sanitized county names


This database has `r nrow(harm.df)` observations. Each observation corresponds to an event occurrence.

To determine the most harmful events to human health, we checked the variables related to human health, which are "fatalities" and "injuries".

To determine the most harmful events to economy, we checked the variables related to economic measures, from "propdmg" through "cropdmgexp". 

Also, in order to analyse various occurrences of the same event, we measured the duration of the event, its magnitude and where the event occurred (state and county name).


This is a really big database whose data has been being registered by a lot of different people since 1950. Thus, as expected, there are variations on how people registered events. 

For example, the string "snow" was used to register a lot of events. They are the same type of event, but count as different:

This is why we decided to filter those events: we grouped them by its common strings.


